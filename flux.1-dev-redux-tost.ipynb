{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content\n",
    "!pip install torchsde einops diffusers transformers accelerate peft timm kornia aiohttp\n",
    "!apt install -qqy\n",
    "\n",
    "!git clone https://github.com/comfyanonymous/ComfyUI /content/ComfyUI\n",
    "!git clone https://github.com/ltdrdata/ComfyUI-Manager /content/ComfyUI/custom_nodes/ComfyUI-Manager\n",
    "\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/FLUX.1-dev/resolve/main/flux1-dev.sft  -d /content/ComfyUI/models/unet -o flux1-dev.sft\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/FLUX.1-dev/resolve/main/clip_l.safetensors -d /content/ComfyUI/models/clip -o clip_l.safetensors\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/FLUX.1-dev/resolve/main/t5xxl_fp16.safetensors -d /content/ComfyUI/models/clip -o t5xxl_fp16.safetensors\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/FLUX.1-dev/resolve/main/ae.sft -d /content/ComfyUI/models/vae -o ae.sft\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/FLUX.1-dev/resolve/main/flux1-redux-dev.safetensors -d /content/ComfyUI/models/style_models -o flux1-redux-dev.safetensors\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/google/siglip-so400m-patch14-384/resolve/main/model.safetensors -d /content/ComfyUI/models/clip_vision -o clip_vision.safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content/ComfyUI\n",
    "\n",
    "import os, shutil, json, requests, random, time\n",
    "from urllib.parse import urlsplit\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "from nodes import NODE_CLASS_MAPPINGS\n",
    "from comfy_extras import nodes_flux, nodes_model_advanced, nodes_custom_sampler\n",
    "\n",
    "UNETLoader = NODE_CLASS_MAPPINGS[\"UNETLoader\"]()\n",
    "DualCLIPLoader = NODE_CLASS_MAPPINGS[\"DualCLIPLoader\"]()\n",
    "VAELoader = NODE_CLASS_MAPPINGS[\"VAELoader\"]()\n",
    "CLIPVisionLoader = NODE_CLASS_MAPPINGS[\"CLIPVisionLoader\"]()\n",
    "LoadImage = NODE_CLASS_MAPPINGS[\"LoadImage\"]()\n",
    "StyleModelLoader =  NODE_CLASS_MAPPINGS[\"StyleModelLoader\"]()\n",
    "\n",
    "ModelSamplingFlux = nodes_model_advanced.NODE_CLASS_MAPPINGS[\"ModelSamplingFlux\"]()\n",
    "StyleModelApply = NODE_CLASS_MAPPINGS[\"StyleModelApply\"]()\n",
    "CLIPVisionEncode = NODE_CLASS_MAPPINGS[\"CLIPVisionEncode\"]()\n",
    "CLIPTextEncode = NODE_CLASS_MAPPINGS[\"CLIPTextEncode\"]()\n",
    "FluxGuidance = nodes_flux.NODE_CLASS_MAPPINGS[\"FluxGuidance\"]()\n",
    "RandomNoise = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"RandomNoise\"]()\n",
    "BasicGuider = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"BasicGuider\"]()\n",
    "KSamplerSelect = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"KSamplerSelect\"]()\n",
    "BasicScheduler = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"BasicScheduler\"]()\n",
    "SamplerCustomAdvanced = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"SamplerCustomAdvanced\"]()\n",
    "EmptyLatentImage = NODE_CLASS_MAPPINGS[\"EmptyLatentImage\"]()\n",
    "VAEDecode = NODE_CLASS_MAPPINGS[\"VAEDecode\"]()\n",
    "\n",
    "with torch.inference_mode():\n",
    "    unet = UNETLoader.load_unet(\"flux1-dev.sft\", \"default\")[0]\n",
    "    clip = DualCLIPLoader.load_clip(\"google_t5-v1_1-xxl_encoderonly-fp8_e4m3fn.safetensors\", \"clip_l.safetensors\", \"flux\")[0]\n",
    "    clip_vision = CLIPVisionLoader.load_clip(\"google--siglip-so400m-patch14-384/model.safetensors\")[0]\n",
    "    style_model = StyleModelLoader.load_style_model(\"flux1-redux-dev.safetensors\")[0]\n",
    "    vae = VAELoader.load_vae(\"ae.sft\")[0]\n",
    "\n",
    "def download_file(url, save_dir, file_name):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    file_suffix = os.path.splitext(urlsplit(url).path)[1]\n",
    "    file_name_with_suffix = file_name + file_suffix\n",
    "    file_path = os.path.join(save_dir, file_name_with_suffix)\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    with open(file_path, 'wb') as file:\n",
    "        file.write(response.content)\n",
    "    return file_path\n",
    "\n",
    "@torch.inference_mode()\n",
    "def generate(input):\n",
    "    values = input[\"input\"]\n",
    "\n",
    "    input_image1 = values['input_image1']\n",
    "    input_image1 = download_file(url=input_image1, save_dir='/content/ComfyUI/input', file_name='input_image1')\n",
    "    input_image2 = values['input_image2']\n",
    "    input_image2 = download_file(url=input_image2, save_dir='/content/ComfyUI/input', file_name='input_image2')\n",
    "    positive_prompt = values['positive_prompt']\n",
    "    seed = values['seed']\n",
    "    steps = values['steps']\n",
    "    guidance = values['guidance']\n",
    "    sampler_name = values['sampler_name']\n",
    "    scheduler = values['scheduler']\n",
    "    max_shift = values['max_shift']\n",
    "    base_shift = values['base_shift']\n",
    "    width = values['width']\n",
    "    height = values['height']\n",
    "\n",
    "    if seed == 0:\n",
    "        random.seed(int(time.time()))\n",
    "        seed = random.randint(0, 18446744073709551615)\n",
    "    print(seed)\n",
    "\n",
    "    image1 = LoadImage.load_image(input_image1)[0]\n",
    "    image2 = LoadImage.load_image(input_image2)[0]\n",
    "    conditioning_positive = CLIPTextEncode.encode(clip, positive_prompt)[0]\n",
    "    conditioning_positive = FluxGuidance.append(conditioning_positive, guidance)[0]\n",
    "    clip_vision_conditioning1 = CLIPVisionEncode.encode(clip_vision, image1)[0]\n",
    "    style_vision_conditioning1 = StyleModelApply.apply_stylemodel(clip_vision_conditioning1, style_model, conditioning_positive)[0]\n",
    "    clip_vision_conditioning2 = CLIPVisionEncode.encode(clip_vision, image2)[0]\n",
    "    style_vision_conditioning2 = StyleModelApply.apply_stylemodel(clip_vision_conditioning2, style_model, style_vision_conditioning1)[0]\n",
    "    unet_flux = ModelSamplingFlux.patch(unet, max_shift, base_shift, width, height)[0]\n",
    "    noise = RandomNoise.get_noise(seed)[0]\n",
    "    guider = BasicGuider.get_guider(unet_flux, style_vision_conditioning2)[0]\n",
    "    sampler = KSamplerSelect.get_sampler(sampler_name)[0]\n",
    "    sigmas = BasicScheduler.get_sigmas(unet_flux, scheduler, steps, 1.0)[0]\n",
    "    latent_image = EmptyLatentImage.generate(width, height)[0]\n",
    "    samples, _ = SamplerCustomAdvanced.sample(noise, guider, sampler, sigmas, latent_image)\n",
    "    decoded = VAEDecode.decode(vae, samples)[0].detach()\n",
    "    Image.fromarray(np.array(decoded*255, dtype=np.uint8)[0]).save(f\"/content/flux.1-dev-redux-{seed}-tost.png\")\n",
    "\n",
    "    result = f\"/content/flux.1-dev-redux-{seed}-tost.png\"\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = { \n",
    "        \"input\": {\n",
    "        \"input_image1\": \"https://files.catbox.moe/1ba4mh.png\",\n",
    "        \"input_image2\": \"https://files.catbox.moe/mkcchl.jpg\",\n",
    "        \"positive_prompt\": \"cute anime\",\n",
    "        \"negative_prompt\": \"blurry\",\n",
    "        \"seed\": 0,\n",
    "        \"steps\": 20,\n",
    "        \"guidance\": 30,\n",
    "        \"sampler_name\": \"euler\",\n",
    "        \"scheduler\": \"normal\",\n",
    "        \"max_shift\": 1.15,\n",
    "        \"base_shift\": 0.50,\n",
    "        \"width\": 1024,\n",
    "        \"height\": 1024\n",
    "    }\n",
    "}\n",
    "image = generate(input)\n",
    "Image.open(image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ComfyUI-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
